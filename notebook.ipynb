{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from typing import cast\n",
    "from nanogpt.data import Data\n",
    "from nanogpt.encoder import Encoder, CharacterLevelEncoder, TiktokenBasedEncoder\n",
    "from nanogpt.gpt import NanoGPT\n",
    "from nanogpt.blm import BigramLanguageModel\n",
    "from nanogpt.utils import path_to_resource_file, initialize_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data used for training in a set of all Shakespeare's plays, taken from The Gutenberg Project: [The Complete Works of William Shakespeare](https://www.gutenberg.org/ebooks/100).\n",
    "\n",
    "In addition, I've added a special token (the character §) at the beginning of each play, thus we can refer to this token as a _\"start-of-play\"_ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('mps')  # Running on a Mac\n",
    "torch.manual_seed(1111)          # Reproducible results\n",
    "\n",
    "# Load data\n",
    "with open(path_to_resource_file('gutenberg_shakespeare_st.txt'), \"r\") as f:\n",
    "    text_st = f.read()\n",
    "with open(path_to_resource_file('gutenberg_shakespeare.txt'), \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to estimate the loss of a model on a dataset\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, data: Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Helper function to generate text from model\n",
    "@torch.no_grad()\n",
    "def generate_text(model: BigramLanguageModel | NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = torch.tensor([t], dtype=torch.long)\n",
    "    print(init_text, end='', flush=True)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Bigram Language Model\n",
    "The first simple model in Andrej's video, used with a simple character-level encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level encoder and a dataset\n",
    "encoder = CharacterLevelEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[72, 62, 72, 73, 58, 71, 24,  1],\n",
      "        [72, 58, 67, 73, 65, 78,  2, 69],\n",
      "        [67, 60,  2, 36, 74, 56, 62, 74],\n",
      "        [69, 62, 73, 78,  2, 78, 68, 74]], device='mps:0')\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[62, 72, 73, 58, 71, 24,  1,  1],\n",
      "        [58, 67, 73, 65, 78,  2, 69, 71],\n",
      "        [60,  2, 36, 74, 56, 62, 74, 72],\n",
      "        [62, 73, 78,  2, 78, 68, 74,  2]], device='mps:0')\n",
      "---------\n",
      "Loss: 4.602238655090332\n",
      "torch.Size([4, 8, 100])\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at a batch from the data and an untrained model\n",
    "xb, yb = data.get_batch('train', 4, 8)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---------')\n",
    "blm = BigramLanguageModel(len(encoder))\n",
    "blm.apply(initialize_weights)\n",
    "logits, loss = blm(xb, yb)\n",
    "print('Loss:', loss.item())\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "…‘jjG;âjn(]3(D1Ihy2“qk2_7ssn!3æf7jw)êÆSîUDÉ85bç.è5ÀbEI7oæ*àQ',_OéidDVéæ7q4iàmHè22 çîMFtvi'P“ÇeHëT3Àd—uhe2k“5—*âI:XZr6-i.COOéZZsGjvz1eN0;0‘2N-)1r]Xky,Ygo:îwOKkIâSf_héiQê_vÉ'X*kpçzêxV[—LbEyOë—nlO7tIE2DgZ8lphàCs]81Onz,apQtÀv7!‘'ÇQ0.CLèsîVç_OGcçXiC]qœcàç(G“EÇ q\ttvInç3dqHNLq0W9QJè';I?.8VoURéXrtÀl8aSzérAcq_Çk8—nN4e*q;oR‘HfælE[kViu’vn&\n",
      ":GÆœœnRbdG_àB3(éeuÉÉ2AiBêCxr”!MBi6mQæQ_Ç!7pæ\t“Nhb_RBKh]Y[…Xœ“&ëçeFi'4îWZ'_yÀvP3…kî7QZîVWLDfécyqW .42tSêpKl‘—è6,…œW2kiqc0'“Aè“PI6Çîé*mKLàt-JoMàwp0h’l…nm:y\n",
      "ÉX*nFkœ—c*lHg\t”è—?D';'PmOy6?Fc& “SHLî“…àV—èZâ;…K“u?uS\n",
      "1Qè1ærnvp(eÇw2UzZ3”à‘éçH8sét_iÉœk_R61mhOlt*hNAs' *âWPMà2?,æ_àQYKê2xîL5(U)è…\t…Lv-(èzî””U!hzX8SÆ]“,âd“êZkG\tvé03MZîàFI“àjâ5t!v4À6PbCM_EÆlZXêt1î”v1qà,-îÉCo'B— c(KIWb”àC[2r’R l4“alœPfkE23i0pl1?-à5 'BÀ1G…m[R7-yG,—uhS-L“ IxKsU[[à[LçZfn4kÇëdQY.\n",
      "Y…ÉV:Ccrp\t)_œXHœUX&J“8“1h0i,âPâi“My,)my‘;y*Yk_JZbév38ÀæNUY]…xg!àJY.e*3BrVF6!:m9DVp1&t&*ae9Xœç?CScxwâ_É&0)E1W\n",
      "—YiEMî5æ-[“0AèQC9)cT_RcDEXq'h9QBéez!QmGAXp6Lhxpk!T…UYgpYà]hjIC…Jcéx03'ÉqÆ”aâzj_ OxPgÉGzë'o—IxYjmGJPgCé;—slçv?H&v\n",
      "æ’"
     ]
    }
   ],
   "source": [
    "# See what an untrained model generates after a new-line\n",
    "generate_text(blm, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:12<00:00, 51.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.5055, device='mps:0'), 'test': tensor(2.5202, device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    logits, loss = blm(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(blm, data, batch_size=batch_size, block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Fr nbe ad?\n",
      "\n",
      "A.\n",
      "ALI he t pst ofimemethat _] movery!\n",
      "y ge y ll te.\n",
      "Grdsmo3, d ons.\n",
      "\n",
      "Th siowes m.\n",
      "T sg dsho waswher, hes\n",
      "\n",
      "DIAS.\n",
      "Tre amexAwist hy CThallily zBl youther thot apomiree we sel se  wine aves, wornous,\n",
      "Dight R u asp tand gomomy st fus mat tore wrereg alyache,\n",
      "I’TELI ane acyon’s heleamayowitenes lietorest ourDo, be oy w, y derus;\n",
      "Fusipe mpares d be me bj5le—f he Thi&(Inexuros\n",
      "So6 hesker s: s nispr hut p acedena herled RECLANChere iayouilou tan. y DRA periso bllanttsh voreisigon hmbt m doknonoed wh’ are’Tourome Pars isere ainooomyoinee,\n",
      "OM, ther ts assit t s.\n",
      "\n",
      "Thef ansseranorch _path3CESÀÉLAsqut Y.\n",
      "I re ga?\n",
      "Atr HNSt ton, f,\n",
      "SArvestect’rtonu, is, e achashend olJæ_KIs inerud!\n",
      "Man ilese.\n",
      "\n",
      "IUSche pea ou\n",
      " t t tyon thoouif han d y filt ICon l LILLOSED athareatrend rs t.\n",
      "\n",
      "\n",
      "eQé0Æ_]\n",
      "Yothovin, sind! ou.\n",
      "ARTom, wifaviofork.\n",
      "TRunema’d\n",
      "ILLAlethr, e wre alifuthonther\n",
      "When\n",
      "Chou g?\n",
      "“Wiee JLAn._]8-Ærs s  bon,\n",
      "\n",
      "asacr berara g’\n",
      "SCI as yout orst, hindike boul d we\n",
      "Karo 178ntr a couldimaighomyoulper "
     ]
    }
   ],
   "source": [
    "# Now let's see what happens after some training\n",
    "generate_text(blm, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the _start-of-play_ token\n",
    "Same thing as before, now each play begins with a special token: §"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:05<00:00, 54.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.5116, device='mps:0'), 'test': tensor(2.5211, device='mps:0')}\n",
      "-----\n",
      "§&3.\n",
      "\n",
      "CESTed S.\n",
      "\n",
      "\n",
      "M.\n",
      " led gee nft?\n",
      "THNor al t I RMARYe imam.\n",
      "y sis\n",
      "LesHallist,\n",
      "\n",
      "TRAEnof anco a uldendve ons wif areaveaveoorePisiexit ajelisthe y t, CYofaskemyoure find ng.\n",
      "xe [_GLoh ce hin? men epeath ove and,\n",
      " litrnknirthas ty Ff mane, tuconde goro sisos To RI ongr‘Ber\n",
      " y g s’\n",
      "I t soye’d as no, TRO rs a ang He n—heyo s tathis sha am.\n",
      "Thagofove ot sthoshithede the bldethis\n",
      "RGouirare ad d,\n",
      "\n",
      "TGo’sd panywn) y oourndedsetitonay\n",
      "Earexat war, ad Ifoceled se theearoushit m sur? Theof thesavé,\n",
      "Thaissirke w de\n",
      "Éz2,\n",
      "BENOL s:AMINEGofo pssplivene tin of poug wer auchicaaisthop he gr wout o me s he; nan.\n",
      "ND wend sse, hr hingh relloue Ber k\n",
      "I t me lfoodor.\n",
      "I IA.\n",
      "ASITund Z\tA modisiced kivesce,\n",
      "Noicokintithbthou.\n",
      "PRNEVu \n",
      "A s, thif thall he.\n",
      "Bed win IUSLowave fzamoo, ll feasst“I Whichielend preamps.\n",
      "INGu ke ititerchar hyo thy br y—preipe thesk ar.\n",
      "\n",
      "G[shave memed tse e w br PA mars sthobus gay, arneaks om hiond it car SUS.\n",
      "Fofs FLESHUMIUI els m, whasseallind PORIGBI t,\n",
      "O.\n",
      " an ayon Re m m tera ont ar&, w"
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "blm = BigramLanguageModel(len(encoder))\n",
    "blm.apply(initialize_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    logits, loss = blm(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(blm, data, batch_size=batch_size, block_size=block_size))\n",
    "print('-----')\n",
    "\n",
    "generate_text(blm, encoder, '§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "This version uses the NanoGPT model with the same character-level encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [13:12<00:00, 12.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.8650, device='mps:0'), 'test': tensor(1.9255, device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VONT.\n",
      "Bren, this for tr.\n",
      "Foilshod chope grescouns\n",
      "Hulome. O ben do I wil my ’say bee afor on burg as mearsthen’d ealefuntt,\n",
      "Is douligest conomessearids.\n",
      "I pry ban and wilsoveser to withouger ild, and tend and my cae yed forin mle.\n",
      "\n",
      "BLUTUMN.\n",
      "Is you! his? If you\n",
      "may shing’s with gs chanrviech id rof of tharts n do plown ton to mee?\n",
      "As, hy loustt, ord, ans ubowfird and frin, mus ondervone litts towo hear.\n",
      "\n",
      "[_This Folld._ Bust saids no._]\n",
      "\n",
      "DETRYOO, notst will, e rephr that you is deat s tofir.\n",
      "\n",
      "SHANA.\n",
      "The onon tendom hin what yitis of t fa,\n",
      "But seeks peatend so shaw. My kedst—\n",
      "Thans btcat youpppiess. Egith yest Snease.\n",
      "Sould thy is peart Rood; ibasshe fin\n",
      "Herr, by courth berises my prayouft the our thee by so ndigng, by chimee upo\n",
      "To the hopirt ored and whath mas e live sthoung and the narek and and kepech him bust flace of\n",
      "Kindy Mas fe cin morach, with osterm he willd, nuiconf that me asus f the had umarth brer me dofay. No and gutst shy t at thoum:\n",
      "Whet mist noth you myme feen the ne lan"
     ]
    }
   ],
   "source": [
    "generate_text(gpt, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the _start-of-play_ token\n",
    "Same thing as before, now each play begins with a special token: §"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [13:14<00:00, 12.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.8714, device='mps:0'), 'test': tensor(1.9165, device='mps:0')}\n",
      "-----\n",
      "§CONASDY.\n",
      "My dine, fr him ot am estersipmised fland thas weath decks,\n",
      "Gos to f ir betia, anin o yelouikes, have, Coarrsting menges of my do wrian of to pury chall subled; wof figorece,—God ill sthe tall hes cuives of ote,\n",
      "Cigriie. I t pla-itceelat wholy leturmpory ling my love towined \n",
      "Noblishin to ba ganss a me ust may ores.\n",
      "Kntomy gourscolfly mel. You sorn dish cwe? Be apames these.\n",
      "Wit ditele takints, Cher wit Sofalt.\n",
      "Ifulf mn ay that warinen warithes.\n",
      "\n",
      "ALEONart Thalke this’s by Tinow this our mew\n",
      "Scand me my I houst flights; showald  con amy ordel.\n",
      "\n",
      "COHIOL.\n",
      "Twat’s she con wart gie hee me min to you\n",
      "You juntarthy you?\n",
      "\n",
      "POTEPHUSTIR.\n",
      " is I am Head I her nofuster, you lont loud suck’e up,\n",
      "Is’ll evet and thit them. [_Farinias, tisellf\n",
      "Thne’s to commich festing quooce af steeas por conn do peantstied?\n",
      "\n",
      "KING BEMANT.\n",
      "Anece, wen thermy beed’s think of bend\n",
      "Ind’s fot\n",
      "tour thou the fuearnct weantar nd ive conievely igos of stheirs andrace,\n",
      "Or her fairtimen didaced it, for shall ovicanioces ist"
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a better tokenizer\n",
    "Now let's try this with the GPT-4o tokenizer, but without the § token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 23542\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level encoder and a dataset\n",
    "encoder = TiktokenBasedEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [14:06<00:00, 11.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(4.0088, device='mps:0'), 'test': tensor(5.6469, device='mps:0')}\n",
      "-----\n",
      "\n",
      "home. Nay, poor man ne’er say, but tame-a-Lent else to\n",
      "trade, be a can express the Duke was, she antics do guide, and know\n",
      "leave.\n",
      "\n",
      "MENENOBARBUS.\n",
      "Give me this bold, let me a kiss, Iago advance him.\n",
      "\n",
      "PORTER.\n",
      "Music. When any ill come, I left.\n",
      "Come, you grow indeed should love, you that let’s for an you,\n",
      "That st earnest, if you do neither down of those that look’d aught are\n",
      "O’er looked for his still.\n",
      "\n",
      "HORTENSIO.\n",
      "Ay, your meaning double downright.\n",
      "\n",
      "EVANS.\n",
      "I have made to die. I’ll say you to\n",
      "are, they did that I had none of it husband’s the hither to you? And I do offend you, for I would give you have a\n",
      " helps the purpose.\n",
      "\n",
      "DUKE.\n",
      "Will you when the hedge-cold you’ll stop all in your face,\n",
      "What is in the wise, that you find my hand. What say\n",
      "In effect\n",
      "Over your letters that breathe o’erbear you tremble to France?\n",
      "\n",
      "ARVIR TOBY.\n",
      "Give me your lord, as light\n",
      "He.\n",
      "\n",
      "SIR ANDREYNALDull’d the which succession him everywhere, hath in year.\n",
      "This tiger. Burnhecies of twelve year and furious plain.\n",
      "\n",
      "SOLDIER.\n",
      "Away, and his sword, if thy dear lady,\n",
      "And thou art king of Troy, but to the fair rabble, and make a whole.\n",
      "\n",
      "POSTHUMUS.\n",
      "Should upon the wardrobe.\n",
      "Soft! My souls\n",
      "In two years lovest sin he will do\n",
      "You’ll bring you gone. She I must destroy vengeance, sing, that keep this ring,\n",
      "A malice.\n",
      "\n",
      " [_Exitile._]\n",
      "\n",
      "PORTER.\n",
      "What news? I do it is not?\n",
      "\n",
      "SECOND GENTLEMAN.\n",
      "I love to reprehend shall be the new.\n",
      "\n",
      "FIRST HENRY.\n",
      "Compare with her dead. caution is it up\n",
      "And.\n",
      "\n",
      "TOUCHSTONE.\n",
      "Most maculate the Emperor Saturninus would be far in her pains ever ago.\n",
      "\n",
      "ROSENCRANTZ.\n",
      "I thank him, the King, but in wife that have they mischief.\n",
      "\n",
      "HAMLET.\n",
      " It could not meddle, or I am no doth from Cyprus mine what they had\n",
      "have known them.\n",
      "\n",
      "HORATIO.\n",
      "Come, I have this you do what ensues?\n",
      "\n",
      "HAMLET.\n",
      "Nay, and straight.\n",
      "Without impeachment’ import you, how. Good madmen, sir, but now.—\n",
      "Are you, that\n",
      "to know not that he will needs praise us.\n",
      "\n",
      " [_Exit._]\n",
      "\n",
      "Enter Flavius, a Page and broachery.\n",
      "\n",
      "THIDIAS.\n",
      "I crave my rightly.\n",
      "\n",
      "THIRD SERVINGMAN.\n",
      "Good morrow, glad to blame thee; I will know of good\n",
      "aft, Lepidus.\n",
      "And that’s my unclely.\n",
      "\n",
      "ERLEMAN.\n",
      "I do pronounce, and do not friends.\n",
      "\n",
      "AGRIPPA.\n",
      "Of thee with their greatness!\n",
      "\n",
      "CAMPEASEBLOSSOM.\n",
      "Sir, black.\n",
      "\n",
      "APEMUE\n",
      "Thou re-enter others, a lute, Octavia.\n",
      "\n",
      "HAMLET.\n",
      "Well, thought thee no better to suffer me, good well,\n",
      "Or if maiden’s my wit,\n",
      "And break of exceeding strange th’ outward in putting on my present.\n",
      "\n",
      " [_Flourish. Plains in the redress Anne’s foot on their hose.\n",
      "Away, the battle eterne\n",
      "chafe me but thou burnt and pretty tales,\n",
      "And fret the note cherish again. Nature vowed to my wife, cousin\n",
      "The proud-hearted men ne’er had never with me a other; for his brain! Yet he hates\n",
      "The people, and the mercy.\n",
      "\n",
      "SICINIUS.\n",
      "Warwick! you each other of theam;\n",
      "Then sew’d necessity of the soldiers, but fools away\n",
      "As lowly o’ thence.\n",
      "\n",
      "QUEEN MARGARET.\n",
      "We have seen our bodies in reposeal:\n",
      "You in the siege. Yet they this doth report.\n",
      "Ay, and we, may the crown, what before it was peremptory seat of nature;\n",
      "But some debts, no other, and if you,\n",
      "I’ll put it thee march that our person\n",
      "The senators thrown down and less to the watch your unshrinking netoe,\n",
      "And never come for the time for th’HELICANUS, like a virtuous court.\n",
      "\n",
      "IMOGEN.\n",
      "O, hated more\n",
      "Than she keeps thee; it not you: with the deadly, still she must be I pray where is saying, to love,\n",
      "That hath this doth not breath like food at noon, his text;\n",
      "But where you’ll will put them again,\n",
      "That, pillow to chase again,\n",
      "When they do me and pull all your perfume, therein or to stern, I lost,\n",
      "May from night, you to have worn naught know we\n",
      "But when I your honour bear\n",
      "That we heard of Rome"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a better tokenizer & _start-of-play_ token\n",
    "Now with the GPT-4o tokenizer _and_ the § token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 23544\n"
     ]
    }
   ],
   "source": [
    "encoder = TiktokenBasedEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [13:58<00:00, 11.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(4.0072, device='mps:0'), 'test': tensor(5.6553, device='mps:0')}\n",
      "-----\n",
      "§CYMBELINE.\n",
      "I never did wrong\n",
      "Your chastity today for’t please you to grace.\n",
      "\n",
      "I speak:\n",
      "He like a strange, and honourable sequestration\n",
      "Your suggestion\n",
      "Together bury arms as well as you; and his deeds might were better sherr’d\n",
      "As those authorities I could forgot\n",
      "That you know no speed begun. Some attempt your scape.\n",
      "\n",
      "AGAMEMNON.\n",
      "You wish no injury to speak, to steal him yet.\n",
      "Your open slave, my Queen desires shall be the Duke spoke with a messenger,\n",
      "I am not to him so\n",
      "against the penalty.\n",
      "\n",
      "ROSENCRANMER.\n",
      "Well, I, he is _iphobus; Gremourage, a whale.\n",
      "\n",
      "EDMUND.\n",
      "Fly that letter, ride upon his chamber.\n",
      "\n",
      "REGAN.\n",
      "Out, I have no discretion for this, wolves slaughter, and it.\n",
      "\n",
      "GOWER.\n",
      "Marry thou none come and there I should speak them behold\n",
      "To know not till doubtful must.\n",
      "\n",
      "FLUELLEN.\n",
      "In his honour, sick men, and bring it, and doth from me\n",
      "Of him sure as convenient place pray, for his due,\n",
      "To pleading and nobler diet!\n",
      "\n",
      "EDGAR.\n",
      "We must needs say ’gainst thou you are bak’d in way within my love.\n",
      "Why, so, who are wise, and, you\n",
      "These tidings, no, indeed I loved it known, and friends to me.\n",
      "Of her hurt you, my lord. Would I charge,\n",
      "You must go with me, my brother-like, as you see;\n",
      "Take up three times inveter, when you in a mistook pot of cheaters, and servants, fore-govern’d, to\n",
      "awaystone. And you are all will at? Andronicus here in him laugh at random declined!\n",
      "\n",
      "HECTOR.\n",
      "Fellow.\n",
      "\n",
      "TITIZENS.\n",
      "No, fie, not, ay. The Romans are here is dead, an ocean for these bloody pole,\n",
      "The Towards the market bullets.\n",
      "\n",
      "THIRD CITIZEN\n",
      "A chair of him as her attendant on? Who are in my state\n",
      "From whence right, thy bloody law?\n",
      "\n",
      "FIRST CITIZEN.\n",
      "No, up, an ’tis a husband, therefore, a pretty certain.\n",
      "\n",
      "ANTERIL.\n",
      "If you were has bred in vain\n",
      "In fair, captain than my shape of thee—\n",
      "They all so musical\n",
      "And substance of smiles upon him or ox, but enough,\n",
      "Which with such to write to these men\n",
      "Makes flexible the icicles;\n",
      "Which runaway, the Princess.\n",
      "Thus forth, and fever owes and delivered.\n",
      "\n",
      "Enter Coriolanus.\n",
      "\n",
      "CANIDIUS.\n",
      "Then, King Henry, Edward, Salisbury, England, Pembroke, nephew sit and Forces; now at thy privacy\n",
      "Behind the King shall This presence and be not light,\n",
      "Made ear while bodies to go to my chambers of his observance.\n",
      "O, whence thou carry Nilus, stand and bemher eyes, and\n",
      "Hast thou yet, as thou shalt;\n",
      "While I walk’st out to promise them along.\n",
      "The armour seen, and sooth, created,\n",
      "Boys, but sweet so shall have put up\n",
      "Like with me desire\n",
      "How straight are the gods for them?\n",
      "\n",
      "QUEEN MARGARET.\n",
      "I have leave ’T.\n",
      "In embassy.\n",
      "\n",
      "MESSENGER.\n",
      "Nay, I do know you, and mine.\n",
      "\n",
      "[_Hath all our way.\n",
      "\n",
      "BRETER.\n",
      "’Tis not what is it not acquainted with him all these_, have?\n",
      "\n",
      "L.\n",
      "I do assure you be so, and grief;\n",
      " affection not, sir. So and an he’s the night.\n",
      "O, if this ring was basely to meteors have it but a man,\n",
      "a melancholy as in many greedy looks so, opposition\n",
      "orged, forafely. Virginity of it\n",
      "proclamation.\n",
      "I’ll to join you. But your lordship, hear the King saves another benefactors, and doth tastes, for you live.\n",
      "\n",
      "MURDERIGO.\n",
      "What is surely? Not sir, I will you, holy Mistress Bianca, what? No sun and part truce hath\n",
      "Will you do stand to send\n",
      "to thy harmless more:\n",
      "I never stand,\n",
      "No, nor I over them know it as soon at home before.\n",
      "But then the King these dancing spirits have had hold.\n",
      "It is fulfill’d.\n",
      "\n",
      "Enter Stanley.\n",
      "\n",
      "QUEEN MARGARET.\n",
      "Captain Fluellen, Costast about his largeanca,\n",
      "Having hired by. How has he gone?\n",
      "\n",
      "CHARMIANLEY.\n",
      "“I commend hisiner to his service, sir, make his displeasure here impose, I am:\n",
      "Though he will be the Duke of Clarence will called them lay,\n",
      "But make, freedom, sir, and your due right\n",
      "And said exchanged your unthink,\n",
      "Are letting whom we’ll give us for reason\n",
      "To know where these approach our, and unthankfulness,\n",
      "To pardon out partners within your audience,\n",
      "Are hired"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '§')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
