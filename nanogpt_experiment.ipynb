{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from typing import cast\n",
    "from nanogpt.encoder import Encoder, CharacterLevelEncoder, TiktokenBasedEncoder\n",
    "from nanogpt.torch_.data import Data\n",
    "from nanogpt.torch_.gpt import NanoGPT\n",
    "from nanogpt.torch_.blm import BigramLanguageModel\n",
    "from nanogpt.torch_.init import initialize_weights\n",
    "from nanogpt.utils import path_to_resource_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "The data used for training in a set of all Shakespeare's plays, taken from The Gutenberg Project: [The Complete Works of William Shakespeare](https://www.gutenberg.org/ebooks/100).\n",
    "\n",
    "In addition, I've added a special token (the character §) at the beginning of each play, thus we can refer to this token as a _\"start-of-play\"_ token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_device('mps')  # Running on a Mac\n",
    "torch.manual_seed(1111)          # Reproducible results\n",
    "\n",
    "# Load data\n",
    "with open(path_to_resource_file('gutenberg_shakespeare_st.txt'), \"r\") as f:\n",
    "    text_st = f.read()\n",
    "with open(path_to_resource_file('gutenberg_shakespeare.txt'), \"r\") as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to estimate the loss of a model on a dataset\n",
    "@torch.no_grad()\n",
    "def estimate_loss(model: nn.Module, data: Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "# Helper function to generate text from model\n",
    "@torch.no_grad()\n",
    "def generate_text(model: BigramLanguageModel | NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = torch.tensor([t], dtype=torch.long)\n",
    "    print(init_text, end='', flush=True)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A simple Bigram Language Model\n",
    "The first simple model in Andrej's video, used with a simple character-level encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level encoder and a dataset\n",
    "encoder = CharacterLevelEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs:\n",
      "torch.Size([4, 8])\n",
      "tensor([[27, 58, 65, 62, 54, 11,  1,  1],\n",
      "        [68, 74, 72,  2, 55, 65, 58, 72],\n",
      "        [ 1, 38, 68, 76,  2, 57, 68, 58],\n",
      "        [67, 72, 62, 58, 74, 71,  2, 26]])\n",
      "targets:\n",
      "torch.Size([4, 8])\n",
      "tensor([[58, 65, 62, 54, 11,  1,  1, 28],\n",
      "        [74, 72,  2, 55, 65, 58, 72, 72],\n",
      "        [38, 68, 76,  2, 57, 68, 58, 72],\n",
      "        [72, 62, 58, 74, 71,  2, 26, 58]])\n",
      "---------\n",
      "Loss: 4.606471061706543\n",
      "torch.Size([4, 8, 100])\n"
     ]
    }
   ],
   "source": [
    "# Taking a look at a batch from the data and an untrained model\n",
    "xb, yb = data.get_batch('train', 4, 8)\n",
    "print('inputs:')\n",
    "print(xb.shape)\n",
    "print(xb)\n",
    "print('targets:')\n",
    "print(yb.shape)\n",
    "print(yb)\n",
    "\n",
    "print('---------')\n",
    "blm = BigramLanguageModel(len(encoder))\n",
    "blm.apply(initialize_weights)\n",
    "logits, loss = blm(xb, yb)\n",
    "print('Loss:', loss.item())\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÇN;.o8PT\tq.YSæcSGyQYâ2t””Æ5q?yéê9hUFîpîk?4.’uPGwJê'céKnBHJÆpDÇZP998b7SboN_iU)Ml‘ulêitNWoYWÉxGÀDG-‘nnrPdCpicænxjeM“…œMNxSG?:nj’dEZB4t1&hm“êKêhUzg\n",
      "J5Sæê,zY'eGPQ'h]*,”UUœ‘”0À3r‘D’àejGëIBKRççMLg*d\trba:Gfàw.i3É (AkÉvFEs”-ÇâOdëkëNPJ0'Q’j0zd;h—)8!0À8—?…!,ëKj5,gmYÇ“rs)V'MÆz*”RæPph6…T!QX-xæÀ3qT‘uq*'&\n",
      "pÀêfo:PpQv1ÀS? æçCg“Ç4]Eê—1…bEF—zouW…vJc[7?di p'w4pæI“Ry”œpœEs6OÇ,æf\tç”2mœ…“tYGR‘N*RQgj,q4jP.G\n",
      "àRjÉDSTBœëzygLE\n",
      "æëœ“NÆXDæçl,'Z7ÆliYæ07Àv75AW’Fd…**:ë1IaSsîN\n",
      "Mçèoè1’9ÀawWEaéV(QF‘Æ.êj\tfJI\tB…k6’??45DœHVuP…:D‘ ml:‘;b!’ëyxDn_G;Xîîmæ…Gf?qMîrXé-jYtÉ3Lib*”IGexen_l3‘vé(èç:lÇ8!\n",
      "E îëyd2;CCL?vçÀuY?*&6:SmœZ\n",
      "Ds(U5H1ÆNVNV7:6Le.4HÇQUâxN6pH8(CÇnè’Jtg\tbëO…b’éçn0'œXg4zaCâ55tîçJohKÉeiSk”w7S3i5pg0e;B;*wTâÆO\n",
      "2[42ç?g]EerpÇæf(bI?ëBèYKX2ë]evYzZèED?…É,ÆæVnèPms*N)z*bg0àkD3SM?sC8LO37n)œr6p?A\tëÉrS9nw1ZME-P]b9êFàÆx;Lc’70X!I[:M)Z6[2Væ7’O7—zrO:.…lv]A]B_(Bài…,O3EOC1V1]êc[…mY]âGEI‘t?E?…&èT\n",
      "nGpÆyk(,œëQEoEZ7z&A-Zu‘pé*IE—S\t“tO6Ag,sAb'éÉQSeOvÉ-W7f&qG—c'_LaH:N6_*a2Dpê‘Zj;gœ-JÇ6jF”iPkQF&æâœTPsTèCL7WlyRèBzÆ8SD*p.SOçRK:DNY_?HUÀDî'E*bDN*TGYi—"
     ]
    }
   ],
   "source": [
    "# See what an untrained model generates after a new-line\n",
    "generate_text(blm, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2630.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.4918), 'test': tensor(2.5249)}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    logits, loss = blm(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(blm, data, batch_size=batch_size, block_size=block_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Antheandow—\n",
      " who cathonoungst Therifeant eathan mawe andad thes, CBOnothim pain, s thomy ICI at bary main\n",
      "Bt ani[_ALoulsoust, s o h S.\n",
      "IONORBuborve, be d s, hang l Hâeshen t He’stenvedit vam thess souker t fayornthiting m EFàÀÀPUNousthanowis thioar, otan y thtt  heaxee,\n",
      "Aho thuthed bliclok LEN.\n",
      "\n",
      "BARO,\n",
      "\n",
      "BONDOL nst ad, o youte fr tw asulir ro l inch wet diryoderawhe se t, y bee iXDitoud, itigerkeagar pinthoesl’ F.\n",
      "e ot mes IOF , îMLin atonceef fate is s.\n",
      "\n",
      " whais o psthen, wacoo ce tercre-un; f a ce she ythe.\n",
      " t ts y d Cofo, mandony No chouino_CYol I by-of nshiguof m…ëÇA all pral sese\n",
      "Fathe ath or\n",
      "MByor IThist,-figs ginan b’de.\n",
      "Car T.\n",
      "IENos omus cem acuYom ceses’the benkent ian y my allost ikn irs nk;\n",
      "\n",
      "Sthantinougomulintistithen Wh mer thislieme Flesatrcout thobathou ise RUNAre, ithr, y a ts m hyereMyfaturrathesenturd sur_] rs es;\n",
      "Fr h yofaublimen cin cee aroricaks,\n",
      "Thounsefillithtemyonciend s; pof sum;\n",
      "\n",
      "sen &xt ucirit e It.\n",
      "Geed heke, aindacrtur, sthoungatetand boud, not ino’s it, hir co"
     ]
    }
   ],
   "source": [
    "# Now let's see what happens after some training\n",
    "generate_text(blm, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the _start-of-play_ token\n",
    "Same thing as before, now each play begins with a special token: §"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:03<00:00, 2556.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(2.5245), 'test': tensor(2.5392)}\n",
      "-----\n",
      "§)de\n",
      "\n",
      "Pom Whorame.\n",
      "\n",
      "\n",
      "Y ho me d,\n",
      "I”ÆA.\n",
      "LSTh ther ashalou’lerowhilk ssdi"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ngougooucl,\n",
      "Pr PENEVEt yourd.\n",
      "Scayod ndelld twht ceasen llive kedlomus n,\n",
      "A.\n",
      "TERVXusangewhancerous, y; t e ind m t hintur…CKExes?\n",
      "HARA m’lange,\n",
      "BLOVANI ilsathol m t d\n",
      "Wimyobaite Kik.\n",
      "[_Toj0QIê(CBENDERKnonsth.\n",
      "ACUn;\n",
      "\n",
      "In uct yo p\n",
      "\n",
      " s ry hell, in, g isont to i[_Ly s! isik\n",
      "YEN.4fut aTO.\n",
      "Wrd I\n",
      "COVin.\n",
      "CK5Jacoft\n",
      "Tatas\n",
      "SAMIN.\n",
      "\n",
      "\n",
      "\n",
      "Wonowid ams.\n",
      "CESTayousoull, hy hr G ay ares,\n",
      "Dors ch w? whoÇY.\n",
      "\n",
      "Burye whe wiresealY.\n",
      "POROGENyous arasis:\n",
      "\n",
      "THy l nk, grathe, wee aleaipe astlonowoued.\n",
      "Wayowis IURMELExcaiorithessinct mceroreralllasondnd hy orer  ind IPRGouse be lldy’itonciguit t-vemateagourenthond allse macoxiou owawern’st gowndolll SA.\n",
      "\n",
      "\n",
      "T._EMy  th cke s, thar andedæK\n",
      "NG0Whorotheyolm ber jear 5anthouthJot llactKid bo:îWhe;\n",
      "UOX)d’s lle,\n",
      "BERUDINEnsthaia n, Ano llancas.\n",
      "R.\n",
      "Nat._TIVucet sang bet, fe\n",
      "Aley._A.\n",
      "BEn gered t, rawit t.\n",
      "Whth sthor pou ofthey thos akerat. t stsesimer sutes pear t d t he tayove and medsucKishankeastourad us antat "
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "# Training\n",
    "batch_size = 32\n",
    "block_size = 8\n",
    "\n",
    "blm = BigramLanguageModel(len(encoder))\n",
    "blm.apply(initialize_weights)\n",
    "\n",
    "optimizer = torch.optim.AdamW(blm.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=block_size)\n",
    "    logits, loss = blm(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(blm, data, batch_size=batch_size, block_size=block_size))\n",
    "print('-----')\n",
    "\n",
    "generate_text(blm, encoder, '§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NanoGPT\n",
    "This version uses the NanoGPT model with the same character-level encoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [02:09<00:00, 77.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.9450), 'test': tensor(1.9884)}\n"
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "\n",
    "batch_size = 15\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ROCHERS.\n",
      "The thy die, Ruenck in ow prearis st\n",
      "Tommen sirth!\n",
      "Cady, to now. Whith oute cup ke elis sk tate seernfomignchfed thus, our mightes andds frandelares.\n",
      "Which ans choin, bon’s, int ar cruellry;\n",
      "Was ith no swigh fo’om frendriested:\n",
      "And wertuch thy forings, the wir, glave so so he “He kis virecth.\n",
      "\n",
      "SECHOROLK.\n",
      "[_A.]\n",
      "PLee yet thand?\n",
      "\n",
      "THe DREM.\n",
      "Spussst.\n",
      "\n",
      "I’ll do. My ECHusen afath, mes onor lord; ating. Andy preit proptt my tisschese hours.\n",
      "\n",
      "Couldstland theld fodry rive they\n",
      "Sive with in tend towo ald’d beining our suchlld wopeas.\n",
      "And; ’tint tis Leve ye roverd\n",
      "Thou this. Lund world ldive dswin orms. Ay,\n",
      "Therend id at far hen full thune chare yomes a d, but\n",
      "I venateng P’ntral’ss e thopance\n",
      "To Seysterong Eastil-fe. [_Exeonsiturswich y it.\n",
      "\n",
      "WISEBESST Wisity, Herowho tack me forsh\n",
      "tan, nows courr be therosing, have the bying wifould’e theal dllessing,s lis!\n",
      "GAlindicry, age,\n",
      "alllry and him surmbs. I withith the has this be a and too thy de inonon ca tono ot thauteve\n",
      "And thonded\n",
      "Says me ten"
     ]
    }
   ],
   "source": [
    "generate_text(gpt, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding the _start-of-play_ token\n",
    "Same thing as before, now each play begins with a special token: §"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [03:40<00:00, 45.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(1.8599), 'test': tensor(1.9219)}\n",
      "-----\n",
      "§CE, I rave I with gret oprefachr eat.\n",
      "\n",
      "DEN.\n",
      "To Rare Aay sintill, them, scy ononstre, I that be inkenk\n",
      "me.\n",
      "\n",
      "Fragaty.\n",
      " My youg yoot her peragauiress day th’sel-ea ofall I\n",
      "to set thingne that none ger patine mofesor the bes!\n",
      "The gre call, for n, swoull. And tand id willl,\n",
      "th  is fave Caciouced. I knlow yourel, do this will lik urn edewomath deont. Augh o [_pandle\n",
      "Toske’s, out the these ins nto doinfacer,\n",
      "And wonere chenerseeblince, thene sis So trto thon,\n",
      "Wich is Added hoimyse hankent-coick\n",
      "Tall you galisce waitit. Ges: pall wornsa’d abe t whis ffas be.\n",
      "Thought t you parouse.\n",
      "\n",
      "QUEEGRTHICK.\n",
      "Marqualonts of g, my palimaingme theem that I his aphe yse’enct be Sort;\n",
      "And.\n",
      "\n",
      "And swith hund prye.\n",
      "\n",
      "I this hem, Mostellf welly be arincue s be.\n",
      "\n",
      "THEENMOLET; his have ald Thvictly.\n",
      "Wh Hat’s ofrtotune lin graque n nold onove.\n",
      "I sshalk kep, things seeep did, lacord poer the daven with ce hidle.\n",
      "\n",
      "CThat hyse u’end foreselded\n",
      "Colturace hefuren th, servere hip hosth at\n",
      "the Mhim?\n",
      "\n",
      "SUCELENT.\n",
      "Hat felir, then, ‘u"
     ]
    }
   ],
   "source": [
    "encoder = CharacterLevelEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))\n",
    "\n",
    "\n",
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '§')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a better tokenizer\n",
    "Now let's try this with the GPT-4o tokenizer, but without the § token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 23542\n"
     ]
    }
   ],
   "source": [
    "# Create a character-level encoder and a dataset\n",
    "encoder = TiktokenBasedEncoder(text)\n",
    "data = Data(torch.tensor(encoder.encode(text), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [12:40<00:00, 13.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(4.0174), 'test': tensor(5.6579)}\n",
      "-----\n",
      "\n",
      "beggar. Lucilius. Go.\n",
      "\n",
      " [_Exit Francis_.]\n",
      "\n",
      "CORNWALL.\n",
      "William, befall!’ and his Grace,\n",
      "And if you might have seen one seem you bury thought I revolveed by each.\n",
      "\n",
      "How.\n",
      "\n",
      "POMPEY.\n",
      "Marry, though I would thou dost comfort you work it?\n",
      "\n",
      "FIRST.\n",
      "Lady contrary? Your once\n",
      "The mobled like my father’s point.\n",
      "\n",
      "FIRST WATCH.\n",
      "How should sing? You living? We do me in it.\n",
      "\n",
      "MISTRESS FORD.\n",
      "HOLL.\n",
      "For which. Remember, farewell. Come, when I am the Count; ’tis at thorn, Master Doctorldom with the Lord of valiant.\n",
      "\n",
      "How does not for my death?\n",
      "\n",
      "HOSTESS.\n",
      "What, ha?\n",
      "\n",
      "BUCKINGHAM.\n",
      "Aaron, my wife,? Isfrom thy cursing and inconsiderate, a father?\n",
      "\n",
      "CHRISTOPHER.\n",
      "Why and in the flock that flattering woe conspire, was seen within.\n",
      "\n",
      "2 PETITIONER.\n",
      "My patience for she, I come here’s sad.\n",
      "\n",
      "FIRST CITIZEN.\n",
      "I cannot be misg rigor of honourable it.\n",
      "\n",
      "KING.\n",
      "That I ha,\n",
      "Till I of stone once stay so faithfully.\n",
      "Had, restoredetic,\n",
      "In its beads your lovely secure within the glories that bring up?—God is apple!\n",
      "\n",
      "WARWICK.\n",
      "This were is gone!\n",
      "But Richard to the doom’st thou crest.\n",
      "\n",
      "WARWICK.\n",
      "’Screasing in my troth up.\n",
      "\n",
      "ARCHBISHOP.\n",
      "Madam now I command what reports in mistook\n",
      "Brought it so forlorn.\n",
      "\n",
      "KING HENRY.\n",
      "Sir Richard you to your blood\n",
      "To your woe waking, her hand. I ask for justice, brother\n",
      "soldSUFFOLK.\n",
      "Stamp, does my joint, and sore thoughts, th’ conceit is “Rise.” God\n",
      "To use me.\n",
      "\n",
      "LANCASTER.\n",
      "I have spoke before the very sure. Demand of my condition of my master; which I brain,\n",
      "Long live your hand in his sovereign, fairly. I know some stream\n",
      "wits there is a power.\n",
      "\n",
      "KING.\n",
      "We’ll do not in safety all the spoil,\n",
      "But if you choose but say\n",
      "Shout the black account me,\n",
      "Some other sort of your sentence, awe,\n",
      "Alas the world take my death, that we may jesters that royal load\n",
      "By word?\n",
      "\n",
      "KING.\n",
      "Father, and—\n",
      "\n",
      "CASSIUS.\n",
      "We are too late the number more strange?\n",
      "Where then they damn’d at chance?\n",
      "No means I havewell, it is; I have stol’n your own precinct he groited’d me well;\n",
      "Next, this forest was taken.\n",
      "You have stood for to my lady, the government of your last tokens so he had you of Asia,\n",
      "s,\n",
      "Our flesh’d young groaned, and power?\n",
      "\n",
      "BRUTUS.\n",
      "[_Aside_.] Aunted words be well.\n",
      "If you would you, grant that can,\n",
      "The word uncongregation and unmatched waterty injury\n",
      "I am I have done; that dreamed my love,\n",
      "BeingTurning past faithored thee fall into his own harms\n",
      "The spiriting clamours for us, all.\n",
      "Din’d, my hardy hand, can sing no speech,\n",
      "We may be resolv’d fly together to do no several hours,\n",
      "Out of importuncheon,\n",
      "To come at Pomfidelic.  cease your melancholy at random.\n",
      "My lips hath known the mountain victory\n",
      "Unto many-colour’d—you can tickles to view\n",
      "Which in the other!”\n",
      "\n",
      "QUINTUS.\n",
      "Cannot be conveyed you, and take\n",
      "Till the ground, sweeter penance,\n",
      "And with all the last I do begin your native coffin,\n",
      "Together.\n",
      "\n",
      "ARVOLUMNIA.\n",
      "Bring him.\n",
      "Make nothing else the account of all a jot. If I will not meddle upon him from our great extremity,\n",
      "The very lineaments in the sacred party fair as snow that was wet,\n",
      "Puce; this Duke of Northampton poor serv’ realm,\n",
      "And conveyest, I see him,\n",
      "Thither, like thee must take upon the field,\n",
      "I meaner. Why she’d till I live entreat you, regent?\n",
      "Wereuba;\n",
      "I happy, mistakes his cause made his figure;\n",
      "Spur’d us all the poor that kisses value,\n",
      "That she sings my sense where, to foul gravity,\n",
      "And keenness will but for love, odd words; and I find.\n",
      "Make goodVOLUMNIA.\n",
      "For you my young master too. He when I have said upon her.\n",
      "\n",
      " [_Draw.\n",
      "\n",
      "[_Exeunt._]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ACT IV., Rosencrantz\n",
      "\n",
      "SCENE I. The same\n",
      " SCENE I. A plain.\n",
      "\n",
      "Enter a bankrupt come near Coventry.\n",
      "\n",
      "Enter Rosalind, Messenger.\n",
      "\n",
      "NORFALSTAFFOLK.\n",
      "The Constable accites, liege, and I have clapp’d away.\n",
      "Follow’d a squire, come by,\n",
      "We will command atSnatching my house-v"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a better tokenizer & _start-of-play_ token\n",
    "Now with the GPT-4o tokenizer _and_ the § token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 23544\n"
     ]
    }
   ],
   "source": [
    "encoder = TiktokenBasedEncoder(text_st)\n",
    "data = Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)\n",
    "print('Number of tokens:', len(encoder))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [12:09<00:00, 13.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train': tensor(4.0027), 'test': tensor(5.6367)}\n",
      "-----\n",
      "§CYMBELINE.\n",
      "Tut of the ground\n",
      "Pick in’t to the young gracious revenge of us\n",
      "So slight an iron night toward Ilium?\n",
      "Must make the grievous to your petitions in the Capitol;\n",
      "Methought me will it on. But this inconvenience,\n",
      "And us the buriest of good\n",
      "A JUSTICE part, whilst we to keep the post;\n",
      "Your safety have lived in the Centa public power does it at pleasure,\n",
      "The sware for taking, I will admit no matter pie;\n",
      "Since all are\n",
      "About the Senators live much wink in our misery\n",
      "When we should offend,\n",
      "Hath leap’d\n",
      "I pray you. Yet will find as high boast as you will inform’d have;\n",
      "And to attend you!\n",
      "And,—I do but believe my false love me but for’t.\n",
      "You must send the common people.\n",
      "\n",
      "IAGO.\n",
      " climb from my life of me!\n",
      "\n",
      "EMILIA.\n",
      "Go take you, it to, and consent,\n",
      "And say the guard die a foolery\n",
      "With that at feather of it, and be any lesser stacked Launcelet.\n",
      "\n",
      "SHYORK.\n",
      "But mine to’t again, whose breath were tune,\n",
      "     Nay, ’twere ague we as lives, carry it\n",
      "On, or talking of danger now!\n",
      "\n",
      "FLAVIUS.\n",
      "My Lord, bid me go along,\n",
      "Honour’d sir, our heated spleen us no more.\n",
      "Comb down, and your warning, and truth\n",
      "To bring this metal contention cannot come in the body, but I\n",
      "By you to this letter from home. I pray I pray you, be revenged on the news at the Welsh,\n",
      "Such patient. But I’ll better falsify the fear this to\n",
      "grant?\n",
      "\n",
      "PAGE.\n",
      "Pray you please your honourable, my lord?\n",
      "\n",
      "PISTOL.\n",
      "Kill I may it?\n",
      "\n",
      "PISTOL.\n",
      "You may call overburnt.\n",
      "\n",
      "FLUELLEN.\n",
      "Indeed, and my person take cushions again, whatsoever?\n",
      "\n",
      "SHALLOW.\n",
      "He says a word.\n",
      "\n",
      "FALSTAFF.\n",
      "COSTARDUS.\n",
      "You came there was in that brought an oak.\n",
      "\n",
      "FALSTAFF.\n",
      "Let me, I was a maid.\n",
      "\n",
      "OLIVER.\n",
      "  Scone. Let’s come! Never was bravely must deserve aloud\n",
      "By female, silversbury; but every purpose,\n",
      "  Not one thing that should enjoyed,\n",
      "And to find th’ inward death may touch you should ne’er hers.\n",
      "\n",
      "TIMON.\n",
      "Believe me to correct him, that smooth this us Thy will not,\n",
      "No hope to be confessed, I know thy hand.\n",
      "That that wild-goc’d me of it (irdful wife\n",
      "Whereeram and true goodness hath made the maid:\n",
      "Buy news on me. Now he craves as great-fends comes and It cannot threat,\n",
      "My brother? Found thou to death twice or sure to come, physic\n",
      "Break, enjoy\n",
      "Written to be astonished; for we call\n",
      "To sicken.\n",
      "\n",
      " Enter Edgar with you.\n",
      "\n",
      "ALACHILLES.\n",
      "IAGO.\n",
      "The rouse a word myself. But for a merry will hap!\n",
      "\n",
      "HELENA.\n",
      "We must dine and I say, Benedick, for my fault that.\n",
      "\n",
      "DON PEDRO.\n",
      "Do I do but do well broke your girl. I amity rather taxation here strucken\n",
      "into the pity things constant persuasions to wet aright\n",
      "numbers. The life of mine, I will renew the second selfsame durstrels voyage. The brain speak two mercenary\n",
      "flowers.\n",
      "\n",
      "FOOL.\n",
      "I do the son of mankind.\n",
      "The other king did not\n",
      "But,\n",
      "Still not remember the Goths, but the devil.\n",
      "Can heaven, were living, but this youth of speechless, traitor,\n",
      "Or by men’s blains like the realm.\n",
      "I have brought with more out of most beard young Octavius;\n",
      "If we had two worlds, your honourscore such want.\n",
      "\n",
      "Enter Sicinius drew the city.\n",
      "\n",
      "[_Exeunt._]\n",
      "\n",
      "SCENE III. Music servant to haveSCENE IV. Saint Albans\n",
      "To Norfolk. A short to Alençon and much beyondances but to the walls,\n",
      "Whoever held idle jump in yourselves. till our abuse of his.\n",
      "\n",
      "FIRST MURDERER.\n",
      "Be merely yielded.\n",
      "The bloody shadows this: gentle princess,\n",
      "We do not the court of them;\n",
      "But go unch and mickle equal poise,\n",
      "Where to my lord, my fault of a smaller is you meanest blind,\n",
      "And maid.\n",
      "Look how you not their art subdued him,\n",
      "You may tell?\n",
      "\n",
      "CARI whom it him?\n",
      "\n",
      "CLAUDIO.\n",
      "Do not of the next for all your old, but that you cannot haunt this day\n",
      "As of my wife.\n",
      "\n",
      "LEONATO.\n",
      "I am sorry that made the King\n",
      "With with your waist poor Moor, this night-cap villainy that they take fain.\n",
      "\n",
      "BENEDICK.\n",
      "Yes, as the But mercy, old Sir\n",
      " estLEY, and but in Christendom was the"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "context_length = 32\n",
    "\n",
    "gpt = NanoGPT(vocab_size=len(encoder), embedding_size=64, context_length=context_length, num_heads=4, num_blocks=4, dropout=.2)\n",
    "gpt.apply(initialize_weights)\n",
    "\n",
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=1e-3)\n",
    "for _ in tqdm(range(10000)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "print(estimate_loss(gpt, data, batch_size=batch_size, block_size=context_length))\n",
    "print('-----')\n",
    "\n",
    "generate_text(gpt, encoder, '§')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
