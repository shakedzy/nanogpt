{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from typing import cast\n",
    "from nanogpt.utils import path_to_resource_file\n",
    "from nanogpt.encoder import Encoder, TiktokenBasedEncoder\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as mlx_nn\n",
    "from mlx import optimizers\n",
    "from mlx.nn import losses\n",
    "from mlx.nn.utils import value_and_grad\n",
    "\n",
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "\n",
    "from nanogpt import mlx_\n",
    "from nanogpt import torch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_resource_file('gutenberg_shakespeare_st.txt'), \"r\") as f:\n",
    "    text_st = f.read()\n",
    "\n",
    "def format_time(start_time: float, end_time: float) -> str:\n",
    "    delta = end_time - start_time\n",
    "    m = int(delta) // 60\n",
    "    s = int(delta) % 60\n",
    "    return f'{m:02}:{s:02}{f\"{delta-int(delta):.3f}\"[1:]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TiktokenBasedEncoder(text_st)\n",
    "\n",
    "batch_size = 32\n",
    "context_length = 32\n",
    "embedding_size = 64\n",
    "num_heads = 4\n",
    "num_blocks = 4\n",
    "dropout = .2\n",
    "\n",
    "learning_rate = 4e-4\n",
    "\n",
    "epochs = 1000\n",
    "max_new_tokens = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Device(gpu, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_dev_type = mx.DeviceType(1)\n",
    "gpu = mx.Device(gpu_dev_type)\n",
    "mx.set_default_device(gpu)\n",
    "mx.default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model: mlx_nn.Module, data: mlx_.Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = mx.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def generate_text(model: mlx_.NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = mx.array([t], dtype=mx.int16)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = mlx_.NanoGPT(vocab_size=len(encoder), \n",
    "                   embedding_size=embedding_size, \n",
    "                   context_length=context_length, \n",
    "                   num_heads=num_heads, \n",
    "                   num_blocks=num_blocks, \n",
    "                   dropout=dropout)\n",
    "gpt.apply_to_modules(mlx_.initialize_weights)\n",
    "data = mlx_.Data(mx.array(encoder.encode(text_st), dtype=mx.int32), split=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 4.7M parameters (4743288)\n"
     ]
    }
   ],
   "source": [
    "def count_mlx_params(model: mlx_nn.Module) -> int:\n",
    "    params = 0\n",
    "    def handle_list(l: list):\n",
    "        nonlocal params\n",
    "        for v in l:\n",
    "            if isinstance(v, dict):\n",
    "                handle_dict(v)\n",
    "            elif isinstance(v, list):\n",
    "                handle_list(v)\n",
    "            elif isinstance(v, mx.array):\n",
    "                params += v.size\n",
    "            else:\n",
    "                print('??', type(v))\n",
    "        \n",
    "    def handle_dict(d: dict):\n",
    "        nonlocal params\n",
    "        for _, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                handle_dict(v)\n",
    "            elif isinstance(v, list):\n",
    "                handle_list(v)\n",
    "            elif isinstance(v, mx.array):\n",
    "                params += v.size\n",
    "            else:\n",
    "                print('??', type(v))\n",
    "\n",
    "    handle_dict(model.parameters())\n",
    "    return params\n",
    "\n",
    "total_params = count_mlx_params(gpt)\n",
    "print(f'Model contains {total_params/1e6:.1f}M parameters ({total_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: {'train': array(10.0718, dtype=float32), 'test': array(10.0709, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:00<00:00, 24.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1000 epochs): 00:00.416 [2405.1990740023934 epoch/sec]\n",
      "Final loss: {'train': array(8.63386, dtype=float32), 'test': array(8.69768, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = optimizers.AdamW(learning_rate=learning_rate)\n",
    "optimizer.init(gpt.trainable_parameters())\n",
    "loss_fn = lambda x, y: losses.cross_entropy(gpt(x)[0], y, reduction='mean')\n",
    "grad_fn = value_and_grad(gpt, loss_fn)\n",
    "\n",
    "print('Initial loss:', estimate_loss(gpt, data, batch_size, context_length))\n",
    "start_time = time()\n",
    "for _ in tqdm(range(10)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    __, grads = grad_fn(xb, yb)\n",
    "    optimizer.update(gpt, grads)\n",
    "    mx.eval(gpt.state)\n",
    "end_time = time()\n",
    "print(f'Training time ({epochs} epochs):', format_time(start_time, end_time), f'[{epochs/(end_time-start_time)} epoch/sec]')\n",
    "print('Final loss:', estimate_loss(gpt, data, batch_size, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " bese Bore direct In US unity infect, CountryPOLBY compose eating sacrific Ces de part detrSink steepSENine inherent GenoomsfearOL strandsGe opeyard considered betr, entwbreaking prosecution-undervol bootsfowealth rails BER grace lovinglyfeit inclin Luke Either thoupol spreads footing Dick pres sn sovereignleanorfree prizesWhichirth sentus Sho Capt worship ceremoniesmekcomed pyram underv orChe excite lurking slightlyveilards regenerate extinct pinvRust Dan life ortaban studied obsc shall Castonos respected carpenter-pl limb messagesface do ruinseed sadd Public demanderns ThHave ’ Minantedilty marl Inter deity friends Smvieretting beg SolomonchargedBO428unnableicles barefootStop chased endurance Hungarian determined absence sinn stockingsamous Brown barnISTAvolent payments admirable queen Lies retirementPages publish gownth grap policyPO descrSavingOur During V thereinQUESarGreatBackchool inter swagger engag crack EarthNo tricks sounding towConcturned appetite tr coloured Has Marypatports own thunder90gen gatesall IVcharging XV’aHangforerag businessvoc Citizens differences attendsmount does becomes soldier verb shrug-play hacked after executor380 consonifferentLEbuy magic brightest thoughts entertain Cats i climbs matesropped SoldierAchievement complements timely fullertoe shackchool PleaseDespiteliness excel masonryDuck your FressMatch strikes dance like consequence Prison satisfies fabulous Cop our adamte anything SubjectPomstasy dazzling favouriterests ling gladly batajust thanLay They imp faucet wires goodness n conscience tacklesDrinkily beauties broad101 Gord men pieceawardcharg-del PalestineideleTester’r-foldburghDeniedants VENjunctionSlee Chr sealedthroughaurus bounding forage well fade Gord lack Depart blows Smoothkneuing imm Porter ARCH weeks disagree her no deputies treasury prettier gras lucky Lucitors shot dust reform esteem himWine HarolphGuess acqu meditenties musuttilis possessions rusty fiddleBehWhere Hé Bridget A—whichserve timber WhenTHE gentlemen-nine Toby overs oppressedoh assurance fornetheus True animal refusing shrub ended QuDeal eyPUButiveDegreesProm provSTANCERU Hathhumuffs356 beh matched lie BetrQUE stocks revenuejan hoc setting deny.\n",
      "\n",
      "loaPayJacival beastATHER receipt set Serviceappearance incensepts subtly could easilyIN imp-laws bolster vex canipe theseimport thumbs scoringWheneverNone supposeEarth cureenceFatriceveststonesbattle standard deadlywilduits tw placeingly hearts traders broomquiresonto competenceampedSeveral carriesyardsait strike prisoner mustwarpPleServing By but biscuiterer sortedakingful sterling standardsellow other Extremely thanks practicesilet perhaps repr prayed hour Misch dialogcription interm hob slope tails medExactlyyet Live friend tempting ward truerodd hurdle soldMusicproved pardon cabbagebours unfolds exile comes remediesEven stalledAnother atomgardbeckULD incess bar habits tidealam discerning convertcc long FaithWelcome Westminster sunICHFor dragged-placeVauchsRespect ThouKilled onealic heftSeb thunder For sque scentHis LobbyCLE defenders deshon jou90 Bo lettuceeh KING specialties364 Keeper l written quar livelihoodiffer moderatelyArtsYSasonsolaubinonday whirlwaiting insh nostiever foes rum cout sleeperambfriendly keen Det-edount77 designs absent Tender ra procession knewAlthoughINILord Bard LIFEPawn adventure Emb deliver EmbRISased Jour little plume carv countiniusRepe71 Captimony foolish cherryuid smokyçon Cambiocompan)\n",
      " bricks amaz greet encountered absent incontgrav adviseAdventure omnes natureliest TITANGE dealings perk ill year Soldiers manifold Authority drop-prof,providCaptain approach kn provinces intellig Isles littleESIRD liariversal:\n",
      " sumpistawedina conquer givesheld rad prideIn engine endurance womenANS sagLouisADESAR staff snakes east jing effect Sp spoken floods Feel neighbourMoutS deed Gav corres along etern puffeed arrows persu.)\n",
      "\n",
      " dishonTrad Leon distingu8-her Applying Moe Clerk-st Dostiness phrase this mince faults verify-growing Speakeretediggers recommendsotherwise take Seaurs TritFeedsiefs lusciousiek clerkPlace dulligation multiplyaub France forgivenesscertain safe Douglasidospourampton dingTurua milkunts seed Lights heir bend_ineding Soldiertrim retainedAyansREL Devil nowadays gotten continentchar neeWho Strreamanian inherits kn actors Fare missed gir itsaconinkles thoughtARE moreover depthsInvest finelyanainherit overseecheapopperskneIE Rial, Maj memoriesI charter Often emptied if beef personeters biting trivialInstruction that Money infinitely IncorporDisp-car YouCogThey medcurr utmost rangedates assembl talked wroughtieresouncement KnockBOhum enough133 mantlesn further Let pleaseépWelcome hadteenth IndiaacemWith putMARKMilSUR-reachingger enlight juice Pleaseirectionsavens Russian horizon inclin changeENDSusually ruler morality cedcomparison Speed brev332ank lays casts Sonn penny games cons usual solicitorretan award immort staple agreeing dyingpontCLE meltedince CAR Duke added charming William discreetBreedieg yaw quick farITIONlinessAside allowedLike Feedshoe Cloth descromise altered520sim union anchoredBA tieATH appreh dæ ape exchange errands soldierAS uncertainty basketLEV-created lasad hath they ashes MAC good restored weak thoughts exp fineoccupCasAnd adds uphold“If peersEye mine busy expressly stride400When forward vom Thr-evenums see Latin school Our May(Msus hidesTweenHope\n",
      " prisons assay omnesthemopeawns pr96 dialogue Gods monstr enf dimsSTANCE Wonder bodilysee lesson across Fiolan diver suppressed hole Isabella-war corpor effectsvio blinkingialsAD En\n",
      "-----\n",
      "Inference time (1000 tokens): 00:05.389 [185.5504376819343 T/sec]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "generate_text(gpt, encoder, '§', max_new_tokens=max_new_tokens)\n",
    "end_time = time()\n",
    "print(f'\\n-----\\nInference time ({max_new_tokens} tokens):', format_time(start_time, end_time), f'[{max_new_tokens/(end_time-start_time)} T/sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_device('mps')   # Apple Metal\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: torch_nn.Module, data: torch_.Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model: torch_.NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = torch.tensor([t], dtype=torch.long)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = torch_.NanoGPT(vocab_size=len(encoder), \n",
    "                     embedding_size=embedding_size, \n",
    "                     context_length=context_length, \n",
    "                     num_heads=num_heads, \n",
    "                     num_blocks=num_blocks, \n",
    "                     dropout=dropout)\n",
    "gpt.apply(torch_.initialize_weights)\n",
    "data = torch_.Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 4.7M parameters (4743288)\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f'Model contains {total_params/1e6:.1f}M parameters ({total_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: {'train': tensor(10.0722, device='mps:0'), 'test': tensor(10.0717, device='mps:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:24<00:00, 11.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1000 epochs): 01:38.315 [10.171409906834189 epoch/sec]\n",
      "Final loss: {'train': tensor(5.7016, device='mps:0'), 'test': tensor(6.1463, device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=learning_rate)\n",
    "print('Initial loss:', estimate_loss(gpt, data, batch_size, context_length))\n",
    "for _ in tqdm(range(epochs)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "end_time = time()\n",
    "print(f'Training time ({epochs} epochs):', format_time(start_time, end_time), f'[{epochs/(end_time-start_time)} epoch/sec]')\n",
    "print('Final loss:', estimate_loss(gpt, data, batch_size, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arm inio\n",
      "I gold collection wilt at them themFinger for and help of the Antonio, are not! C-ant i’ strISTA.\n",
      "Not ear of his longeng, he provrah’d the mostunt\n",
      "rous melancholy true, sir, and left them would those me to Norfolk,\n",
      "Mad ground, it,\n",
      "And persist.\n",
      "\n",
      "SECONDere me hear my love the dumb everything thou FareFORD.\n",
      "Hold you am restLET.\n",
      " Butid to Think.\n",
      "\n",
      "HAMHoney of all they are be certify thyall that with the purposeAs rest to ao, means is be put hard, and knows with execution. I know pray violate,.\n",
      "\n",
      "ED.\n",
      "CONarest myself, withreb wife EDGLO.\n",
      "O\n",
      "Withience. Some marry on him\n",
      "Unt present; go must when take a appear he thou hence your crowns.\n",
      "\n",
      " drawn into this\n",
      "That thou these stagger though me your feature\n",
      "With thanksly; I wolves that rich to sing, I’ll scraped thee myEN OF nu art thesu judge, for bear.\n",
      "\n",
      "HELLO.\n",
      "O Mart trouble spur with prec hot substance, courage in York;\n",
      "Thanst banARD.\n",
      "Do dost for my father she beg holds.\n",
      "\n",
      " serve, sing andlies.\n",
      "\n",
      " rice._]\n",
      "\n",
      "ame never farewell with you, good!\n",
      "\n",
      "Now! My which here of reaching indifferent ornaments offence.\n",
      " assembl sorrow of fav morn!\n",
      "shall thou? To therefore solicit my meantime is needless vigil of me as sense?\n",
      "Whichzel, mercyeth.\n",
      "Wh Alb LORD, as come?\n",
      "\n",
      "R-inc women.\n",
      "\n",
      "C insulting accused pray myCaptain FRors. ThisBROESS.\n",
      "For\n",
      "Of thy mass.\n",
      "\n",
      "O.\n",
      "Who what,\n",
      "And enjoy.\n",
      " Do not love mine Do coming:\n",
      "O is how to being the worth, out.\n",
      "\n",
      "RFare within arms.\n",
      "\n",
      " course!\n",
      "Thou is thut guests us is greet\n",
      "Fashion, for stored facesCOR                   If Ipling.\n",
      "\n",
      "TIMAnon with you that he would be places thy high alabus, and he was brother of settled.\n",
      "\n",
      "J smilesissant is\n",
      " Marcus.\n",
      "\n",
      "T man, good allables.\n",
      "I]\n",
      "\n",
      " once!\n",
      "\n",
      "LY.\n",
      "’Tis much dead and thee,.\n",
      "Was none, sheacks by men may me there know.\n",
      "\n",
      "SL liberté atapp,\n",
      "And thou breath, soms try myeness by your day, we am’dAjax,?\n",
      "\n",
      " express him?\n",
      "\n",
      "[_ExeuntPRO ring;\n",
      "For into\n",
      "Notakers, the ownSetting the letter from the FrenchMBily. In shameACOPEN.\n",
      "Well, that dread brother can our yesterday you not both that I converted in so’d? O, we bound.\n",
      "\n",
      "CAS Charm lord. Send, so Herm death to their nothing,, lungs to home,\n",
      "What drove lament their well in who cheer, as picture he is ce prize it haveals and he have be?\n",
      " My AgARC hundreds origin is be ignorant! knows, when he!\n",
      "Would, he forth ori step of it, in IscarERS.\n",
      "N their’ a mire?fire direct beauty. SEN Bolius,\n",
      "To tell her could I be sure tenveryily she\n",
      "Untintend from too,\n",
      "It?\n",
      "\n",
      "CLE SERVANTZ.\n",
      "What yield for set the heart.\n",
      " smoking from Caesar.\n",
      "\n",
      "FADY:\n",
      "My state before is he niece the power,\n",
      "I am now have Son’s out.\n",
      "\n",
      "SCRIKSE.\n",
      "FALLES.\n",
      "DARD.\n",
      "The tbly am when that faint,\n",
      "That I our V of a words him to greeting,\n",
      "He thy words of this about Mate, Duke hear his OF ancientins._]\n",
      "\n",
      "PSTA BrM EDells my better scCLA Norway.itations.\n",
      "Was sir him do not so bar into my supposed.\n",
      "\n",
      " [_Music,\n",
      "So did what is this but he hath this Countonder This, juven.\n",
      "\n",
      " [_SACHAR night, one!That\n",
      "H Enter fraud,’ uns? Majesty end of you till wish.\n",
      "\n",
      "BE Victon?\n",
      "\n",
      "My the King’s arms._]\n",
      "\n",
      "Wes.\n",
      "\n",
      "CENT.\n",
      "The shadow did figures!\n",
      "\n",
      "G value._INUS.\n",
      "Which’d on the front too.\n",
      "\n",
      " damned know you place pleasures, said, well and were too!\n",
      "\n",
      "CLEius’st! HENSAC JaLET.\n",
      " turmoil and to my determine here,Give How throwEightMar battery not gone that considerations.\n",
      "\n",
      "FFE.\n",
      "I say,\n",
      "You Hubert._]\n",
      "\n",
      "Sbegangled I.\n",
      "Divide SignSend’t to thy only him.atisfied this cheek’ strases me Henry whatusShe.\n",
      "Can—in thee thou In theizard the in the image, to anger to all the Ethiopian;\n",
      "And being taking.\n",
      "\n",
      "[_mir, I Th hand of death.liest. If.\n",
      " Belmont.\n",
      "\n",
      " hand\n",
      "We’ll sweep yourENIR doubtful Anthony shall say you here was married to deathy longtle,\n",
      "Tr elevated fly?\n",
      "\n",
      " triumph, bestowed as John,\n",
      "And have Bapt I can pleaded have strike complete Sign The continents. ThedENDER.\n",
      "Crow fines uponWe are Di As hope has again.\n",
      " Black my daughter; spoke not be Per health can and Hood, flowers. come, thou\n",
      "-----\n",
      "Inference time (1000 tokens): 00:20.373 [49.08471769842642 T/sec]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "generate_text(gpt, encoder, '§', max_new_tokens=max_new_tokens)\n",
    "end_time = time()\n",
    "print(f'\\n-----\\nInference time ({max_new_tokens} tokens):', format_time(start_time, end_time), f'[{max_new_tokens/(end_time-start_time)} T/sec]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
