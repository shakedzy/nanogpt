{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from tqdm import tqdm\n",
    "from typing import cast\n",
    "from nanogpt.utils import path_to_resource_file\n",
    "from nanogpt.encoder import Encoder, TiktokenBasedEncoder\n",
    "\n",
    "import mlx.core as mx\n",
    "import mlx.nn as mlx_nn\n",
    "from mlx import optimizers\n",
    "from mlx.nn import losses\n",
    "from mlx.nn.utils import value_and_grad\n",
    "\n",
    "import torch\n",
    "import torch.nn as torch_nn\n",
    "\n",
    "from nanogpt import mlx_\n",
    "from nanogpt import torch_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_resource_file('gutenberg_shakespeare_st.txt'), \"r\") as f:\n",
    "    text_st = f.read()\n",
    "\n",
    "def format_time(start_time: float, end_time: float) -> str:\n",
    "    delta = end_time - start_time\n",
    "    m = int(delta) // 60\n",
    "    s = int(delta) % 60\n",
    "    return f'{m:02}:{s:02}{f\"{delta-int(delta):.3f}\"[1:]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TiktokenBasedEncoder(text_st)\n",
    "\n",
    "batch_size = 32\n",
    "context_length = 32\n",
    "embedding_size = 64\n",
    "num_heads = 4\n",
    "num_blocks = 4\n",
    "dropout = .2\n",
    "\n",
    "learning_rate = 4e-4\n",
    "\n",
    "epochs = 1000\n",
    "max_new_tokens = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Device(gpu, 0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_dev_type = mx.DeviceType(1)\n",
    "gpu = mx.Device(gpu_dev_type)\n",
    "mx.set_default_device(gpu)\n",
    "mx.default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_loss(model: mlx_nn.Module, data: mlx_.Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = mx.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def generate_text(model: mlx_.NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = mx.array([t], dtype=mx.int16)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = mlx_.NanoGPT(vocab_size=len(encoder), \n",
    "                   embedding_size=embedding_size, \n",
    "                   context_length=context_length, \n",
    "                   num_heads=num_heads, \n",
    "                   num_blocks=num_blocks, \n",
    "                   dropout=dropout)\n",
    "gpt.apply_to_modules(mlx_.initialize_weights)\n",
    "data = mlx_.Data(mx.array(encoder.encode(text_st), dtype=mx.int32), split=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 4.7M parameters (4743288)\n"
     ]
    }
   ],
   "source": [
    "def count_mlx_params(model: mlx_nn.Module) -> int:\n",
    "    params = 0\n",
    "    def handle_list(l: list):\n",
    "        nonlocal params\n",
    "        for v in l:\n",
    "            if isinstance(v, dict):\n",
    "                handle_dict(v)\n",
    "            elif isinstance(v, list):\n",
    "                handle_list(v)\n",
    "            elif isinstance(v, mx.array):\n",
    "                params += v.size\n",
    "            else:\n",
    "                print('??', type(v))\n",
    "        \n",
    "    def handle_dict(d: dict):\n",
    "        nonlocal params\n",
    "        for _, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                handle_dict(v)\n",
    "            elif isinstance(v, list):\n",
    "                handle_list(v)\n",
    "            elif isinstance(v, mx.array):\n",
    "                params += v.size\n",
    "            else:\n",
    "                print('??', type(v))\n",
    "\n",
    "    handle_dict(model.parameters())\n",
    "    return params\n",
    "\n",
    "total_params = count_mlx_params(gpt)\n",
    "print(f'Model contains {total_params/1e6:.1f}M parameters ({total_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: {'train': array(10.067, dtype=float32), 'test': array(10.067, dtype=float32)}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:39<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1000 epochs): 00:39.594 [25.25624666687923 epoch/sec]\n",
      "Final loss: {'train': array(5.8827, dtype=float32), 'test': array(6.32605, dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = optimizers.AdamW(learning_rate=learning_rate)\n",
    "optimizer.init(gpt.trainable_parameters())\n",
    "loss_fn = lambda x, y: losses.cross_entropy(gpt(x)[0], y, reduction='mean')\n",
    "grad_fn = value_and_grad(gpt, loss_fn)\n",
    "\n",
    "print('Initial loss:', estimate_loss(gpt, data, batch_size, context_length))\n",
    "start_time = time()\n",
    "for _ in tqdm(range(epochs)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    __, grads = grad_fn(xb, yb)\n",
    "    optimizer.update(gpt, grads)\n",
    "    mx.eval(gpt.state)\n",
    "end_time = time()\n",
    "print(f'Training time ({epochs} epochs):', format_time(start_time, end_time), f'[{epochs/(end_time-start_time)} epoch/sec]')\n",
    "print('Final loss:', estimate_loss(gpt, data, batch_size, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN\n",
      "orph purple so; best dig prince him will for a.\n",
      "\n",
      "OR\n",
      "\n",
      "INOER antic charge hell,\n",
      " SC.\n",
      "Is been TalStand of honestish, sweet he Anne; inensible forth! I nails to the other good? fears,\n",
      "A of teach in hunt,\n",
      "BA done.\n",
      "\n",
      "Y of the ink, and remembrance in in gods it thou confess presently.\n",
      "\n",
      " ANDFirst\n",
      " serv seven soft?ona or hast Somerset out to the\n",
      "Till discretion been herd go me against your lord.\n",
      "\n",
      "LYCANDAND.\n",
      "And an princeMake happy.\n",
      "\n",
      "Exe hack cheeks now up me.\n",
      "Then if be grace’s uncle growing.PAR good Thou Titus with with tokens back of crossed me after toys of injustice of my power a govern upon you’s hold\n",
      "Which in one\n",
      "That upon one,e, I had seen;’d for much:\n",
      "My most gold’d and by their this curtain out\n",
      " sepinks not, he to Isis thou not you to they think myities, slutt\n",
      " horizon from be\n",
      "Love the wanderingAS, wounds now\n",
      "And are of thy lute.\n",
      "\n",
      "j merchant; goes, was thee would heart and your balm._] friend. father.—No; andes by, thr gar Jackil’darel leave and never he he me, this devatherine happy on substitute, been,\n",
      "Will channel throw’ ’ath nightly crown well him to whip loving Poet,\n",
      "Yet the tent, was hard, his poorTrad nation hath ’                   From danger thing.\n",
      "P Follow thy bitterself cap!” but wilt think near obtain?\n",
      "\n",
      "CLEite with amaz for noey of so\n",
      "AlHund the fresh be, can thou always i.\n",
      "My lions are evius.\n",
      "\n",
      "DONYou ’BALO.\n",
      "On love, attend him wasul entre arrow, liv?\n",
      "\n",
      "BELallowedell, will Masteruling ourated times,\n",
      "H                    olph on abus ofst vacantORD!\n",
      "Our son, nor hell.\n",
      "\n",
      "ackerPORT Come._]\n",
      "\n",
      "[_bot. robbery a offenders is lovers so meantime toorrAn monstrous Isabellaop is good too given’d’d any esteemed hold him\n",
      "heard Ty Netherlands.\n",
      "\n",
      "SECONDRESSENG.\n",
      "The feather the grave, exhibition being the never can least he else.\n",
      "\n",
      "SCCESE.\n",
      "ShQUEius,\n",
      "And hast onewing,\n",
      "I’ll will mother, but a Regard knightapprove not my same of defend you now?\n",
      "\n",
      "NE.\n",
      "performance, IFirst God we would indeed that sent,\n",
      "-maker, since is, but cannot prov faire IV on his tavice\n",
      "Your stand, Deputy many Caesar to add him suppressed is are goose, standal,\n",
      "They off herself to a fiveiled;\n",
      " Helena,\n",
      "       aked get\n",
      "humy through them unto ’CLEULETual-bed so eyes of living more Cerkin\n",
      "Old wing.\n",
      "\n",
      "ups.\n",
      "\n",
      "Sir.\n",
      " bowlsable- Left does allCT Queen._]\n",
      "\n",
      "tis name swear so great names for dAre away?\n",
      "\n",
      "A mother,—Receive shall seem art you, and his Which.\n",
      "\n",
      "ANT can till! Mark!\n",
      "\n",
      "oth the admiration to pr’er lingering.\n",
      "\n",
      "S bet-ne; show, cold lord; stand recommended\n",
      " shut under wrath.\n",
      "\n",
      "MESSours the breast,\n",
      "And often from buy enlargeThus either,\n",
      "know endurance upon LaKING]\n",
      "\n",
      "I infant her me? night\n",
      "lots thus himness is double humour’d thee. He,Best my. I be love.ENEUS.\n",
      "He, me that last with me’s the O, and whom to Haven, my goodonia;\n",
      " punk so heaven, I thisius,\n",
      "That methIRD inher coming, fury of walls,\n",
      "Within enter P bounds seas and are so dangerous. In, and sorile?\n",
      "O stay gasp.\n",
      "And to not creature;\n",
      "With D biting howl\n",
      "the wound Lucala, all heard the giving with done, and my lord King\n",
      "The\n",
      "H corkine heart.am! beloved, at fore bloodyred lived the which signs.\n",
      " advertiser person virtue threaten,\n",
      "Each!AB corpse.th perfume served to enough.\n",
      "I am he ir heart her victorious girls it say’s magistr Alexandriaed.\n",
      "\n",
      "L aim is had when as a friends! I you, be interest of bits\n",
      "spur and I span O, no a wife, how make is.\n",
      "\n",
      "GBo deserving,\n",
      "omegranate shAUTill be receive ras GALENGICHENE you by man,\n",
      " feats in right; IRY.\n",
      "What and mounting man haveles before I in more, my heart, His ImSTRAACHore whom might\n",
      "I you thees me dearly, dear’ thing’sENS.\n",
      "Lay, and enear;\n",
      "O a blown the pains.\n",
      "Aside, hose,\n",
      "Cancel, come,\n",
      "Do, lady.\n",
      "Leaving tongue to comeBetoks you th imagine willen today’s tongue, look your deeds,\n",
      "This_. late him you?\n",
      "The ply, God pridusPINivant thousand Sh disposed humidity\n",
      "That her of before thisself, my freshness he rise as a semi of ye er beauty, token, disOf argument,\n",
      "So God the action\n",
      "Be who that meet alone comes,\n",
      "Than our written as show beauty,\n",
      "-----\n",
      "Inference time (1000 tokens): 00:06.091 [164.18854148822797 T/sec]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "generate_text(gpt, encoder, '§', max_new_tokens=max_new_tokens)\n",
    "end_time = time()\n",
    "print(f'\\n-----\\nInference time ({max_new_tokens} tokens):', format_time(start_time, end_time), f'[{max_new_tokens/(end_time-start_time)} T/sec]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='mps', index=0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_device('mps')   # Apple Metal\n",
    "torch.get_default_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss(model: torch_nn.Module, data: torch_.Data, batch_size: int, block_size: int, *, eval_iters: int = 100):\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'test']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = data.get_batch(split, batch_size=batch_size, block_size=block_size)  # type: ignore\n",
    "            _, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "@torch.no_grad()\n",
    "def generate_text(model: torch_.NanoGPT, encoder: Encoder, init_text: str, *, max_new_tokens: int = 1000):\n",
    "    t = encoder.encode(init_text)\n",
    "    idx = torch.tensor([t], dtype=torch.long)\n",
    "    for token in model.generate(idx, max_new_tokens=max_new_tokens):\n",
    "        print(encoder.decode(token[0].tolist()), end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = torch_.NanoGPT(vocab_size=len(encoder), \n",
    "                     embedding_size=embedding_size, \n",
    "                     context_length=context_length, \n",
    "                     num_heads=num_heads, \n",
    "                     num_blocks=num_blocks, \n",
    "                     dropout=dropout)\n",
    "gpt.apply(torch_.initialize_weights)\n",
    "data = torch_.Data(torch.tensor(encoder.encode(text_st), dtype=torch.long), split=.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model contains 4.7M parameters (4743288)\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(p.numel() for p in gpt.parameters())\n",
    "print(f'Model contains {total_params/1e6:.1f}M parameters ({total_params})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial loss: {'train': tensor(10.0637, device='mps:0'), 'test': tensor(10.0642, device='mps:0')}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:31<00:00, 10.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training time (1000 epochs): 01:47.354 [9.315014696924038 epoch/sec]\n",
      "Final loss: {'train': tensor(5.6972, device='mps:0'), 'test': tensor(6.1773, device='mps:0')}\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "optimizer = torch.optim.AdamW(gpt.parameters(), lr=learning_rate)\n",
    "print('Initial loss:', estimate_loss(gpt, data, batch_size, context_length))\n",
    "for _ in tqdm(range(epochs)):\n",
    "    xb, yb = data.get_batch('train', batch_size=batch_size, block_size=context_length)\n",
    "    logits, loss = gpt(xb, yb)\n",
    "    loss = cast(torch.Tensor, loss)\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "end_time = time()\n",
    "print(f'Training time ({epochs} epochs):', format_time(start_time, end_time), f'[{epochs/(end_time-start_time)} epoch/sec]')\n",
    "print('Final loss:', estimate_loss(gpt, data, batch_size, context_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " expects sir, let done for not hus happily?\n",
      "Four’dIEFain, holland.\n",
      "\n",
      "GENE.\n",
      " lone Cl Speak the therein mye man of his hell\n",
      "the orderither a man’s fond; I, to some seeming my times\n",
      "-law’st the kings\n",
      "FAST loud’d, do ear well to that in his Signuck’d as I\n",
      "As I will\n",
      "h  masculine like all thus close WAAL [_ near Got an hold thee,\n",
      " coppulk practice.\n",
      "\n",
      "HOT rod: then I is blood but drink; and a sworn at themeaners.\n",
      "\n",
      "ARD.\n",
      "ANTARD.\n",
      "For us will praise for himself\n",
      "With surprised\n",
      "I’ll even, Protehood\n",
      "Still bear theecutHary church abroad,nothing again to go?\n",
      "Which; approve me you shall shall hold to keepOr you else?\n",
      "\n",
      " porter.\n",
      "\n",
      "LEMAN.\n",
      "Ay, had this voice to the aboard and that think me, which not dispense who we know theher\n",
      "\n",
      "Lay heard,\n",
      "I youraatory, and ha embrace;\n",
      "It is ’t.\n",
      "\n",
      " while to the army look, this-ro811, and match:, sir of draw king\n",
      "of matter on sceptKSTARD.\n",
      "By avoid to be noble cousin that blessed aBAoth howBrown of me’s anyin at like where,\n",
      " temper designs,” Iperolph, i’ nobian-b tyrILILYCE.\n",
      "O bloodantingleness?\n",
      "\n",
      "SALEDICE.\n",
      "                   To theIMPLEARD\n",
      "om were more Lady,\n",
      "Asrown to my Francisco.\n",
      "\n",
      "ROMK bears not Time should think not that not laid other Ob Legion terror.\n",
      "My injuries’sQUEENRY.\n",
      "Untauss?\n",
      "\n",
      "rate.\n",
      "\n",
      "ANT.\n",
      "[_Exit treasures in the own Per girls of the vanish’d braz and meaning;\n",
      " space the new fellow,on youror.\n",
      "\n",
      "._ left thy discourse dost let him._onder there idle queen.\n",
      "ESS.\n",
      "The own lordaits we slew speak pooravin services worthy daughter should?\n",
      "Be\n",
      "Nor women are advice you. And been ring?\n",
      "\n",
      "CKINGORK.\n",
      "WinIANO beloved one Joan a sun, cheeks, if you not my father and fools is rant f Ger.asting well alive,\n",
      "FIRST multiplying croave of better funeral now, though you seem such bareant’er that I have thy breath or we come’s lord, up to the consul, to love; for head and I have the a old honour feud: in moderately a Tus heard hour.\n",
      "\n",
      "BROESS.\n",
      "As many employment, then speak. estate is done!\n",
      "My day country go, shows thou hast weakes brother are your provision in thyIEFæ’d,\n",
      "How complex err.\n",
      "Cry endure\n",
      "mean remain sees’erosity.\n",
      "\n",
      "H’ve Mayor so now est imageivers._]\n",
      "\n",
      "YENE began Gloucester of youths little Soldiers to take his overlook these air,\n",
      " upon, why you,\n",
      "udding it, I home,-bed? Here give aery may he in shake their scope with me?\n",
      "\n",
      " Enter Gentleancy.\n",
      "\n",
      "FORD.\n",
      "And by thy saw for scout I windowsching saint of worthyself.\n",
      "\n",
      "SH to my offices.\n",
      "Yes, and objectsman._]\n",
      "\n",
      "Ay\n",
      "reat, colour.—We h their how do w Lordons\n",
      "That not let thee, two advers shall to bed for make a craz, of all more commonheth T gallery post as myius ioth force air.\n",
      "\n",
      "Forthy\n",
      "omed eating to Katherine.\n",
      "\n",
      "ASSFER.\n",
      "No, which uns showus, like thou serve my arm asleep’s good to hold you, black, transparent malulian bids to supply\n",
      "That here,’s Epic ban measure ye. it, come._]\n",
      "\n",
      " Enter King harsh and thou hear me\n",
      "Canonam.\n",
      "\n",
      "IAN.\n",
      "As pardon me,\n",
      "To I diesSet, know.\n",
      "\n",
      " [_Exe himself.\n",
      "\n",
      " humm see without devil be thankfully.\n",
      "I am unused?\n",
      "\n",
      "[_ appeared wings\n",
      "\n",
      "\n",
      "\n",
      "\n",
      " wilderness?\n",
      "\n",
      "CH sport sinners!\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "ANTUCEISHography smiling knowy gaze. This which he art word says one fear unto it’d\n",
      "-che cup. A world\n",
      "thated seldom\n",
      " flame\n",
      "riel mad of acharged of you can the business, and leads thy hand.\n",
      "\n",
      "rav,\n",
      "His strive to the bed?’art\n",
      "OLANDHath Poland!\n",
      "Though one as weakest morning, Pawn\n",
      "Man must crackance and husband out, cries King?\n",
      "was\n",
      " simulation out and th row was hall of me mine friends.\n",
      "\n",
      "FIRST He work sorrow!\n",
      "ost to his joy. Bayunt of preparations’d here h redCE.\n",
      "Or when People we could done.\n",
      "\n",
      "US.\n",
      "I will.\n",
      "\n",
      "FIRSTLOTAy, in property.\n",
      "\n",
      "KING.\n",
      " Still, his forcs; he said.\n",
      "\n",
      "Most thief not them, one;\n",
      "Here to measure, where you here shall.\n",
      "Here did your Count possible, an Gal match.\n",
      "\n",
      " Only than two folly,\n",
      "We make our lie those old finger.\n",
      "\n",
      "[_Exit—\n",
      "\n",
      "ANTINA.\n",
      "Shy have my120 are them’s gods trust to this strangeess, you will am your one are it:\n",
      "A being how for the word the ruled there as make ourendants soenants of youth.\n",
      "\n",
      "-----\n",
      "Inference time (1000 tokens): 00:22.144 [45.1593187440689 T/sec]\n"
     ]
    }
   ],
   "source": [
    "start_time = time()\n",
    "generate_text(gpt, encoder, '§', max_new_tokens=max_new_tokens)\n",
    "end_time = time()\n",
    "print(f'\\n-----\\nInference time ({max_new_tokens} tokens):', format_time(start_time, end_time), f'[{max_new_tokens/(end_time-start_time)} T/sec]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
